[Partial Dependence Plots](https://christophm.github.io/interpretable-ml-book/pdp.html) (PDP) are probably the most widespread method for understanding the global effect of a feature on a model's prediction. They show the **average marginal effect** of a feature on the predicted outcome. In other words: Pretend as if the feature you're analyzing had a fixed value, keep everything else as is, calculate the predictions, and average across all samples. Repeat this procedure for all (relevant) values the feature may take and plot the resulting combinations of feature value and average prediction.

PDPs should contain an indication of the feature's distribution (for example, the histogram in the plot here). This helps to evaluate how trustworthy the calculated marginal effect is for a given feature value: Parts of the distribution with very few samples lead to less stable results. The big caveat about PDPs is that they assume the features are not correlated. This is usually not true in practice and can be misleading. [Accumulated Local Effects Plots](https://christophm.github.io/interpretable-ml-book/ale.html) avoid these biases when features are correlated. However, I am not aware of a mature open-source implementation of ALE Plots in Python.