The obvious first question after inspecting the model predictions and performance is: Which features does the model use to make predictions? [Permutation Feature Importance](https://christophm.github.io/interpretable-ml-book/feature-importance.html) is one straightforward way to answer this question: Just randomly shuffle a feature's values and measure how much the model's performance deteriorates. The bigger the performance drop, the more important the feature. To get a somewhat robust estimate of feature importance, each feature is permuted repeatedly and the performance drop is logged on each iteration. The Boxplot on the right shows the distribution of the importance estimates of our top 10 features.

In this case, a single feature seems to outweigh the impact of the rest decisively. That should make us curious. We'll investigate further in the next section...
